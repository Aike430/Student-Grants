{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## todo: \n",
    "# 解决测评函数不能引入cv的问题\n",
    "# 建立cv流程进行调参\n",
    "# 清理异常点(小于0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "train = pd.read_table('train/subsidy_train.txt',sep=',',header=-1)\n",
    "train.columns = ['id','label']\n",
    "test = pd.read_table('test/studentID_test.txt',sep=',',header=-1)\n",
    "test.columns = ['id']\n",
    "test['label'] = np.nan\n",
    "\n",
    "#train_test = pd.concat([train,test])\n",
    "train_test=train\n",
    "\n",
    "del train\n",
    "del test\n",
    "\n",
    "score_train = pd.read_table('train/score_train.txt',sep=',',header=-1)\n",
    "score_train.columns = ['id','college','rank']\n",
    "score_test = pd.read_table('test/score_test.txt',sep=',',header=-1)\n",
    "score_test.columns = ['id','college','rank']\n",
    "score_train_test = pd.concat([score_train,score_test])\n",
    "\n",
    "college = pd.DataFrame(score_train_test.groupby(['college'])['rank'].max())\n",
    "college.to_csv('input/college.csv',index=True)\n",
    "college = pd.read_csv('input/college.csv')\n",
    "college.columns = ['college','total_people']\n",
    "\n",
    "score_train_test = pd.merge(score_train_test, college, how='left',on='college')\n",
    "score_train_test['rank_percent'] = score_train_test['rank']/score_train_test['total_people']\n",
    "train_test = pd.merge(train_test,score_train_test,how='left',on='id')\n",
    "\n",
    "card_train = pd.read_table('train/card_train.txt',sep=',',header=-1)\n",
    "card_train.columns = ['id','pos','place','consume','time','price','rest']\n",
    "#card_test = pd.read_table('test/card_test.txt',sep=',',header=-1)\n",
    "#card_test.columns = ['id','pos','place','consume','time','price','rest']\n",
    "#\n",
    "#card_train_test = pd.concat([card_train,card_test])\n",
    "card_train_test=card_train\n",
    "print \"Read OK!\"\n",
    "\n",
    "##release memery\n",
    "del card_train\n",
    "#del card_test\n",
    "\n",
    "#card_train_test=card_train_test.drop_duplicates()\n",
    "\n",
    "#card_train_test.drop(['place','time'],axis=1,inplace=True)\n",
    "\n",
    "card_train_test.shape\n",
    "\n",
    "card_train_test.head()\n",
    "\n",
    "#card_train_test['pos'].value_counts()\n",
    "\n",
    "for var in ['地点21','地点829','地点818','地点213','地点72','地点283','地点91','地点245','地点65','地点161','地点996','地点277','地点842','地点75','地点263','地点840']:\n",
    "    \n",
    "    place = card_train_test[card_train_test.place == var]\n",
    "    \n",
    "    feature_p = pd.DataFrame(place.groupby(['id'])['pos'].count())\n",
    "    feature_p['%s_sum'%var]=place.groupby(['id'])['price'].sum()\n",
    "    feature_p['%s_avg'%var]=place.groupby(['id'])['price'].mean()\n",
    "    feature_p['%s_max'%var]=place.groupby(['id'])['price'].max()\n",
    "    feature_p['%s_min'%var]=place.groupby(['id'])['price'].min()\n",
    "    feature_p['%s_median'%var]=place.groupby(['id'])['price'].median()\n",
    "    \n",
    "    del place\n",
    "    feature_p.to_csv('input/card_%sfeature.csv'%var,index=True)\n",
    "    feature_p=pd.read_csv('input/card_%sfeature.csv'%var)\n",
    "    feature_p=feature_p.rename(columns={'pos' : '%s_count'%var})\n",
    "    \n",
    "    train_test = pd.merge(train_test, feature_p, how='left',on='id') \n",
    "    del feature_p\n",
    "\n",
    "card_train_test.time = pd.to_datetime(card_train_test.time, format='%Y/%m/%d %H:%M:%S')\n",
    "card_train_test['month'] = card_train_test.time.dt.month\n",
    "card_train_test['weekday'] = card_train_test.time.dt.weekday\n",
    "card_train_test['days_in_month'] = card_train_test.time.dt.day\n",
    "card_train_test['hour'] = card_train_test.time.dt.hour\n",
    "#card_train_test.drop(['place','time'],axis=1,inplace=True)\n",
    "\n",
    "#card_train_test.drop(['place','consume','time'],axis=1,inplace=True)\n",
    "\n",
    "# tips：\n",
    "# - month 分月统**消费**情况 //（特别注意开学和学期结束（补助到账） \n",
    "# - day 统计充钱时间 //工薪家庭定期发放生活费，猜测与补助成负相关\n",
    "# - hour 统计消费情况 \n",
    "# - week 分为周末于工作日统计\n",
    "'''\n",
    "for m in range(1,13):\n",
    "    for d in range(1,32):\n",
    "        for h in range(0,24):\n",
    "            for w in range(0,7):\n",
    "                feature=card_train_test[(card_train_test.month == m)&(card_train_test.weekday == w)&(card_train_test.days_in_month == d)&(card_train_test.hour == h)] \n",
    "                if feature.empty:\n",
    "                    pass\n",
    "                else:\n",
    "                    card = pd.DataFrame(feature.groupby(['id'])['pos'].count())\n",
    "                    \n",
    "                    card['price_sum%d_%d_%d_%d'%(m,d,h,w)] = feature.groupby(['id'])['price'].sum()\n",
    "                    card['price_avg%d_%d_%d_%d'%(m,d,h,w)] = feature.groupby(['id'])['price'].mean()\n",
    "                    card['price_max%d_%d_%d_%d'%(m,d,h,w)] = feature.groupby(['id'])['price'].max()\n",
    "                    card['price_min%d_%d_%d_%d'%(m,d,h,w)] = feature.groupby(['id'])['price'].min()\n",
    "                    card['price_median%d_%d_%d_%d'%(m,d,h,w)] = feature.groupby(['id'])['price'].median()\n",
    "\n",
    "                    card['rest_sum%d_%d_%d_%d'%(m,d,h,w)] = feature.groupby(['id'])['rest'].sum()\n",
    "                    card['rest_avg%d_%d_%d_%d'%(m,d,h,w)] = feature.groupby(['id'])['rest'].mean()\n",
    "                    card['rest_max%d_%d_%d_%d'%(m,d,h,w)] = feature.groupby(['id'])['rest'].max()\n",
    "                    card['rest_min%d_%d_%d_%d'%(m,d,h,w)] = feature.groupby(['id'])['rest'].min()\n",
    "                    card['rest_median%d_%d_%d_%d'%(m,d,h,w)] = feature.groupby(['id'])['rest'].median()\n",
    "                    \n",
    "                    card.to_csv('input/feature%d_%d_%d_%d.csv'%(m,d,h,w),index=True)\n",
    "                    card = pd.read_csv('input/feature%d_%d_%d_%d.csv'%(m,d,h,w)) \n",
    "                    card=card.rename(columns={'pos' : 'count%d_%d_%d_%d'%(m,d,h,w)}) \n",
    "                    train_test = pd.merge(train_test, card, how='left',on='id')\n",
    "                    del card\n",
    "'''\n",
    "\n",
    "for m in range(1,13):\n",
    "    feature=card_train_test[(card_train_test.month == m)&(card_train_test.pos=='POS消费')] \n",
    "    if feature.empty:\n",
    "        pass\n",
    "    else:\n",
    "        card = pd.DataFrame(feature.groupby(['id'])['pos'].count())\n",
    "                    \n",
    "        card['price_sumM%d'%m] = feature.groupby(['id'])['price'].sum()\n",
    "        card['price_avgM%d'%m] = feature.groupby(['id'])['price'].mean()\n",
    "        card['price_maxM%d'%m] = feature.groupby(['id'])['price'].max()\n",
    "        card['price_minM%d'%m] = feature.groupby(['id'])['price'].min()\n",
    "        card['price_medianM%d'%m] = feature.groupby(['id'])['price'].median()\n",
    "\n",
    "        card['rest_sumM%d'%m] = feature.groupby(['id'])['rest'].sum()\n",
    "        card['rest_avgM%d'%m] = feature.groupby(['id'])['rest'].mean()\n",
    "        card['rest_maxM%d'%m] = feature.groupby(['id'])['rest'].max()\n",
    "        card['rest_minM%d'%m] = feature.groupby(['id'])['rest'].min()\n",
    "        card['rest_medianM%d'%m] = feature.groupby(['id'])['rest'].median() \n",
    "\n",
    "        del feature\n",
    "        \n",
    "        card.to_csv('input/featureM%d.csv'%m,index=True)\n",
    "        card = pd.read_csv('input/featureM%d.csv'%m) \n",
    "        card=card.rename(columns={'pos' : 'countM%d'%m}) \n",
    "        train_test = pd.merge(train_test, card, how='left',on='id')\n",
    "        del card\n",
    "    \n",
    "\n",
    "'''\n",
    "for d in range(1,32):\n",
    "    feature=card_train_test[(card_train_test.days_in_month == d)&((card_train_test.pos=='卡充值')|(card_train_test.pos=='圈存转账'))] \n",
    "    if feature.empty:\n",
    "        pass\n",
    "    else:\n",
    "        card = pd.DataFrame(feature.groupby(['id'])['pos'].count())\n",
    "                    \n",
    "        card['price_sumD%d'%d] = feature.groupby(['id'])['price'].sum()\n",
    "        card['price_avgD%d'%d] = feature.groupby(['id'])['price'].mean()\n",
    "        card['price_maxD%d'%d] = feature.groupby(['id'])['price'].max()\n",
    "        card['price_minD%d'%d] = feature.groupby(['id'])['price'].min()\n",
    "        card['price_medianD%d'%d] = feature.groupby(['id'])['price'].median()\n",
    "\n",
    "        card['rest_sumD%d'%d] = feature.groupby(['id'])['rest'].sum()\n",
    "        card['rest_avgD%d'%d] = feature.groupby(['id'])['rest'].mean()\n",
    "        card['rest_maxD%d'%d] = feature.groupby(['id'])['rest'].max()\n",
    "        card['rest_minD%d'%d] = feature.groupby(['id'])['rest'].min()\n",
    "        card['rest_medianD%d'%d] = feature.groupby(['id'])['rest'].median()\n",
    "        del feature\n",
    "            \n",
    "        card.to_csv('input/featureD%d.csv'%d,index=True)\n",
    "        card = pd.read_csv('input/featureD%d.csv'%d) \n",
    "        card=card.rename(columns={'pos' : 'countD%d'%d}) \n",
    "        train_test = pd.merge(train_test, card, how='left',on='id')\n",
    "        del card\n",
    "        \n",
    "'''\n",
    "\n",
    "## hours <7|7|8|9|17|18|19|19>\n",
    "\n",
    "feature=card_train_test[(card_train_test.hour<7)&(card_train_test.pos=='POS消费')] \n",
    "if feature.empty:\n",
    "    pass\n",
    "else:\n",
    "    card = pd.DataFrame(feature.groupby(['id'])['pos'].count())\n",
    "                    \n",
    "    card['price_sumH7-'] = feature.groupby(['id'])['price'].sum()\n",
    "    card['price_avgH7-'] = feature.groupby(['id'])['price'].mean()\n",
    "    card['price_maxH7-'] = feature.groupby(['id'])['price'].max()\n",
    "    card['price_minH7-'] = feature.groupby(['id'])['price'].min()\n",
    "    card['price_medianH7-'] = feature.groupby(['id'])['price'].median()\n",
    "\n",
    "    card['rest_sumH7-'] = feature.groupby(['id'])['rest'].sum()\n",
    "    card['rest_avgH7-'] = feature.groupby(['id'])['rest'].mean()\n",
    "    card['rest_maxH7-'] = feature.groupby(['id'])['rest'].max()\n",
    "    card['rest_minH7-'] = feature.groupby(['id'])['rest'].min()\n",
    "    card['rest_medianH7-'] = feature.groupby(['id'])['rest'].median()\n",
    "\n",
    "    del feature\n",
    "            \n",
    "    card.to_csv('input/featureH7-.csv',index=True)\n",
    "    card = pd.read_csv('input/featureH7-.csv') \n",
    "    card=card.rename(columns={'pos' : 'countH7-'}) \n",
    "    train_test = pd.merge(train_test, card, how='left',on='id')\n",
    "    del card\n",
    "\n",
    "\n",
    "h=7\n",
    "feature=card_train_test[(card_train_test.hour== h)&(card_train_test.pos=='POS消费')] \n",
    "if feature.empty:\n",
    "    pass\n",
    "else:\n",
    "    card = pd.DataFrame(feature.groupby(['id'])['pos'].count())\n",
    "                    \n",
    "    card['price_sumH%d'%h] = feature.groupby(['id'])['price'].sum()\n",
    "    card['price_avgH%d'%h] = feature.groupby(['id'])['price'].mean()\n",
    "    card['price_maxH%d'%h] = feature.groupby(['id'])['price'].max()\n",
    "    card['price_minH%d'%h] = feature.groupby(['id'])['price'].min()\n",
    "    card['price_medianH%d'%h] = feature.groupby(['id'])['price'].median()\n",
    "\n",
    "    card['rest_sumH%d'%h] = feature.groupby(['id'])['rest'].sum()\n",
    "    card['rest_avgH%d'%h] = feature.groupby(['id'])['rest'].mean()\n",
    "    card['rest_maxH%d'%h] = feature.groupby(['id'])['rest'].max()\n",
    "    card['rest_minH%d'%h] = feature.groupby(['id'])['rest'].min()\n",
    "    card['rest_medianH%d'%h] = feature.groupby(['id'])['rest'].median()\n",
    "\n",
    "    del feature\n",
    "            \n",
    "    card.to_csv('input/featureH%d.csv'%h,index=True)\n",
    "    card = pd.read_csv('input/featureH%d.csv'%h) \n",
    "    card=card.rename(columns={'pos' : 'countH%d'%h}) \n",
    "    train_test = pd.merge(train_test, card, how='left',on='id')\n",
    "    del card\n",
    "\n",
    "h=8\n",
    "feature=card_train_test[(card_train_test.hour== h)&(card_train_test.pos=='POS消费')] \n",
    "if feature.empty:\n",
    "    pass\n",
    "else:\n",
    "    card = pd.DataFrame(feature.groupby(['id'])['pos'].count())\n",
    "                    \n",
    "    card['price_sumH%d'%h] = feature.groupby(['id'])['price'].sum()\n",
    "    card['price_avgH%d'%h] = feature.groupby(['id'])['price'].mean()\n",
    "    card['price_maxH%d'%h] = feature.groupby(['id'])['price'].max()\n",
    "    card['price_minH%d'%h] = feature.groupby(['id'])['price'].min()\n",
    "    card['price_medianH%d'%h] = feature.groupby(['id'])['price'].median()\n",
    "\n",
    "    card['rest_sumH%d'%h] = feature.groupby(['id'])['rest'].sum()\n",
    "    card['rest_avgH%d'%h] = feature.groupby(['id'])['rest'].mean()\n",
    "    card['rest_maxH%d'%h] = feature.groupby(['id'])['rest'].max()\n",
    "    card['rest_minH%d'%h] = feature.groupby(['id'])['rest'].min()\n",
    "    card['rest_medianH%d'%h] = feature.groupby(['id'])['rest'].median()\n",
    "\n",
    "    del feature\n",
    "            \n",
    "    card.to_csv('input/featureH%d.csv'%h,index=True)\n",
    "    card = pd.read_csv('input/featureH%d.csv'%h) \n",
    "    card=card.rename(columns={'pos' : 'countH%d'%h}) \n",
    "    train_test = pd.merge(train_test, card, how='left',on='id')\n",
    "    del card\n",
    "                \n",
    "h=9\n",
    "feature=card_train_test[(card_train_test.hour== h)&(card_train_test.pos=='POS消费')] \n",
    "if feature.empty:\n",
    "    pass\n",
    "else:\n",
    "    card = pd.DataFrame(feature.groupby(['id'])['pos'].count())\n",
    "                    \n",
    "    card['price_sumH%d'%h] = feature.groupby(['id'])['price'].sum()\n",
    "    card['price_avgH%d'%h] = feature.groupby(['id'])['price'].mean()\n",
    "    card['price_maxH%d'%h] = feature.groupby(['id'])['price'].max()\n",
    "    card['price_minH%d'%h] = feature.groupby(['id'])['price'].min()\n",
    "    card['price_medianH%d'%h] = feature.groupby(['id'])['price'].median()\n",
    "\n",
    "    card['rest_sumH%d'%h] = feature.groupby(['id'])['rest'].sum()\n",
    "    card['rest_avgH%d'%h] = feature.groupby(['id'])['rest'].mean()\n",
    "    card['rest_maxH%d'%h] = feature.groupby(['id'])['rest'].max()\n",
    "    card['rest_minH%d'%h] = feature.groupby(['id'])['rest'].min()\n",
    "    card['rest_medianH%d'%h] = feature.groupby(['id'])['rest'].median()\n",
    "\n",
    "    del feature\n",
    "            \n",
    "    card.to_csv('input/featureH%d.csv'%h,index=True)\n",
    "    card = pd.read_csv('input/featureH%d.csv'%h) \n",
    "    card=card.rename(columns={'pos' : 'countH%d'%h}) \n",
    "    train_test = pd.merge(train_test, card, how='left',on='id')\n",
    "    del card\n",
    "                \n",
    "h=17\n",
    "feature=card_train_test[(card_train_test.hour== h)&(card_train_test.pos=='POS消费')] \n",
    "if feature.empty:\n",
    "    pass\n",
    "else:\n",
    "    card = pd.DataFrame(feature.groupby(['id'])['pos'].count())\n",
    "                    \n",
    "    card['price_sumH%d'%h] = feature.groupby(['id'])['price'].sum()\n",
    "    card['price_avgH%d'%h] = feature.groupby(['id'])['price'].mean()\n",
    "    card['price_maxH%d'%h] = feature.groupby(['id'])['price'].max()\n",
    "    card['price_minH%d'%h] = feature.groupby(['id'])['price'].min()\n",
    "    card['price_medianH%d'%h] = feature.groupby(['id'])['price'].median()\n",
    "\n",
    "    card['rest_sumH%d'%h] = feature.groupby(['id'])['rest'].sum()\n",
    "    card['rest_avgH%d'%h] = feature.groupby(['id'])['rest'].mean()\n",
    "    card['rest_maxH%d'%h] = feature.groupby(['id'])['rest'].max()\n",
    "    card['rest_minH%d'%h] = feature.groupby(['id'])['rest'].min()\n",
    "    card['rest_medianH%d'%h] = feature.groupby(['id'])['rest'].median()\n",
    "\n",
    "    del feature\n",
    "            \n",
    "    card.to_csv('input/featureH%d.csv'%h,index=True)\n",
    "    card = pd.read_csv('input/featureH%d.csv'%h) \n",
    "    card=card.rename(columns={'pos' : 'countH%d'%h}) \n",
    "    train_test = pd.merge(train_test, card, how='left',on='id')\n",
    "    del card\n",
    "                \n",
    "h=18\n",
    "feature=card_train_test[(card_train_test.hour== h)&(card_train_test.pos=='POS消费')] \n",
    "if feature.empty:\n",
    "    pass\n",
    "else:\n",
    "    card = pd.DataFrame(feature.groupby(['id'])['pos'].count())\n",
    "                    \n",
    "    card['price_sumH%d'%h] = feature.groupby(['id'])['price'].sum()\n",
    "    card['price_avgH%d'%h] = feature.groupby(['id'])['price'].mean()\n",
    "    card['price_maxH%d'%h] = feature.groupby(['id'])['price'].max()\n",
    "    card['price_minH%d'%h] = feature.groupby(['id'])['price'].min()\n",
    "    card['price_medianH%d'%h] = feature.groupby(['id'])['price'].median()\n",
    "\n",
    "    card['rest_sumH%d'%h] = feature.groupby(['id'])['rest'].sum()\n",
    "    card['rest_avgH%d'%h] = feature.groupby(['id'])['rest'].mean()\n",
    "    card['rest_maxH%d'%h] = feature.groupby(['id'])['rest'].max()\n",
    "    card['rest_minH%d'%h] = feature.groupby(['id'])['rest'].min()\n",
    "    card['rest_medianH%d'%h] = feature.groupby(['id'])['rest'].median()\n",
    "\n",
    "    del feature\n",
    "            \n",
    "    card.to_csv('input/featureH%d.csv'%h,index=True)\n",
    "    card = pd.read_csv('input/featureH%d.csv'%h) \n",
    "    card=card.rename(columns={'pos' : 'countH%d'%h}) \n",
    "    train_test = pd.merge(train_test, card, how='left',on='id')\n",
    "    del card\n",
    "                \n",
    "h=19\n",
    "feature=card_train_test[(card_train_test.hour== h)&(card_train_test.pos=='POS消费')] \n",
    "if feature.empty:\n",
    "    pass\n",
    "else:\n",
    "    card = pd.DataFrame(feature.groupby(['id'])['pos'].count())\n",
    "                    \n",
    "    card['price_sumH%d'%h] = feature.groupby(['id'])['price'].sum()\n",
    "    card['price_avgH%d'%h] = feature.groupby(['id'])['price'].mean()\n",
    "    card['price_maxH%d'%h] = feature.groupby(['id'])['price'].max()\n",
    "    card['price_minH%d'%h] = feature.groupby(['id'])['price'].min()\n",
    "    card['price_medianH%d'%h] = feature.groupby(['id'])['price'].median()\n",
    "\n",
    "    card['rest_sumH%d'%h] = feature.groupby(['id'])['rest'].sum()\n",
    "    card['rest_avgH%d'%h] = feature.groupby(['id'])['rest'].mean()\n",
    "    card['rest_maxH%d'%h] = feature.groupby(['id'])['rest'].max()\n",
    "    card['rest_minH%d'%h] = feature.groupby(['id'])['rest'].min()\n",
    "    card['rest_medianH%d'%h] = feature.groupby(['id'])['rest'].median()\n",
    "\n",
    "    del feature\n",
    "            \n",
    "    card.to_csv('input/featureH%d.csv'%h,index=True)\n",
    "    card = pd.read_csv('input/featureH%d.csv'%h) \n",
    "    card=card.rename(columns={'pos' : 'countH%d'%h}) \n",
    "    train_test = pd.merge(train_test, card, how='left',on='id')\n",
    "    del card\n",
    "                \n",
    "\n",
    "feature=card_train_test[(card_train_test.hour>19)&(card_train_test.pos=='POS消费')] \n",
    "if feature.empty:\n",
    "    pass\n",
    "else:\n",
    "    card = pd.DataFrame(feature.groupby(['id'])['pos'].count())\n",
    "                    \n",
    "    card['price_sumH19+'] = feature.groupby(['id'])['price'].sum()\n",
    "    card['price_avgH19+'] = feature.groupby(['id'])['price'].mean()\n",
    "    card['price_maxH19+'] = feature.groupby(['id'])['price'].max()\n",
    "    card['price_minH19+'] = feature.groupby(['id'])['price'].min()\n",
    "    card['price_medianH19+'] = feature.groupby(['id'])['price'].median()\n",
    "\n",
    "    card['rest_sumH19+'] = feature.groupby(['id'])['rest'].sum()\n",
    "    card['rest_avgH19+'] = feature.groupby(['id'])['rest'].mean()\n",
    "    card['rest_maxH19+'] = feature.groupby(['id'])['rest'].max()\n",
    "    card['rest_minH19+'] = feature.groupby(['id'])['rest'].min()\n",
    "    card['rest_medianH19+'] = feature.groupby(['id'])['rest'].median()\n",
    "\n",
    "    del feature\n",
    "            \n",
    "    card.to_csv('input/featureH19+.csv',index=True)\n",
    "    card = pd.read_csv('input/featureH19+.csv') \n",
    "    card=card.rename(columns={'pos' : 'countH19+'}) \n",
    "    train_test = pd.merge(train_test, card, how='left',on='id')\n",
    "    del card\n",
    "\n",
    "## week\n",
    "\n",
    "feature=card_train_test[(card_train_test.weekday<5)&(card_train_test.pos=='POS消费')] \n",
    "if feature.empty:\n",
    "    pass\n",
    "else:\n",
    "    card = pd.DataFrame(feature.groupby(['id'])['pos'].count())\n",
    "                    \n",
    "    card['price_sumWD'] = feature.groupby(['id'])['price'].sum()\n",
    "    card['price_avgWD'] = feature.groupby(['id'])['price'].mean()\n",
    "    card['price_maxWD'] = feature.groupby(['id'])['price'].max()\n",
    "    card['price_minWD'] = feature.groupby(['id'])['price'].min()\n",
    "    card['price_medianWD'] = feature.groupby(['id'])['price'].median()\n",
    "\n",
    "    card['rest_sumWD'] = feature.groupby(['id'])['rest'].sum()\n",
    "    card['rest_avgWD'] = feature.groupby(['id'])['rest'].mean()\n",
    "    card['rest_maxWD'] = feature.groupby(['id'])['rest'].max()\n",
    "    card['rest_minWD'] = feature.groupby(['id'])['rest'].min()\n",
    "    card['rest_medianWD'] = feature.groupby(['id'])['rest'].median()\n",
    "\n",
    "    del feature\n",
    "            \n",
    "    card.to_csv('input/featureWD.csv',index=True)\n",
    "    card = pd.read_csv('input/featureWD.csv') \n",
    "    card=card.rename(columns={'pos' : 'countWD'}) \n",
    "    train_test = pd.merge(train_test, card, how='left',on='id')\n",
    "    del card\n",
    "\n",
    "\n",
    "feature=card_train_test[(card_train_test.weekday>=5)&(card_train_test.pos=='POS消费')] \n",
    "if feature.empty:\n",
    "    pass\n",
    "else:\n",
    "    card = pd.DataFrame(feature.groupby(['id'])['pos'].count())\n",
    "                    \n",
    "    card['price_sumWE'] = feature.groupby(['id'])['price'].sum()\n",
    "    card['price_avgWE'] = feature.groupby(['id'])['price'].mean()\n",
    "    card['price_maxWE'] = feature.groupby(['id'])['price'].max()\n",
    "    card['price_minWE'] = feature.groupby(['id'])['price'].min()\n",
    "    card['price_medianWE'] = feature.groupby(['id'])['price'].median()\n",
    "\n",
    "    card['rest_sumWE'] = feature.groupby(['id'])['rest'].sum()\n",
    "    card['rest_avgWE'] = feature.groupby(['id'])['rest'].mean()\n",
    "    card['rest_maxWE'] = feature.groupby(['id'])['rest'].max()\n",
    "    card['rest_minWE'] = feature.groupby(['id'])['rest'].min()\n",
    "    card['rest_medianWE'] = feature.groupby(['id'])['rest'].median()\n",
    "\n",
    "    del feature\n",
    "            \n",
    "    card.to_csv('input/featureWE.csv',index=True)\n",
    "    card = pd.read_csv('input/featureWE.csv') \n",
    "    card=card.rename(columns={'pos' : 'countWE'}) \n",
    "    train_test = pd.merge(train_test, card, how='left',on='id')\n",
    "    del card\n",
    "\n",
    "\n",
    "##这里的consume是有问题的，consume一列包含刷卡和消费\n",
    "\n",
    "## bash feature\n",
    "card = pd.DataFrame(card_train_test.groupby(['id'])['pos'].count())\n",
    "\n",
    "card['price_sum'] = card_train_test.groupby(['id'])['price'].sum()\n",
    "card['price_avg'] = card_train_test.groupby(['id'])['price'].mean()\n",
    "#\n",
    "card['price_max'] = card_train_test.groupby(['id'])['price'].max()\n",
    "card['price_min'] = card_train_test.groupby(['id'])['price'].min()\n",
    "card['price_median'] = card_train_test.groupby(['id'])['price'].median()\n",
    "\n",
    "card['rest_sum'] = card_train_test.groupby(['id'])['rest'].sum()\n",
    "card['rest_avg'] = card_train_test.groupby(['id'])['rest'].mean()\n",
    "card['rest_max'] = card_train_test.groupby(['id'])['rest'].max()\n",
    "card['rest_min'] = card_train_test.groupby(['id'])['rest'].min()\n",
    "card['rest_median'] = card_train_test.groupby(['id'])['rest'].median()\n",
    "\n",
    "card.to_csv('input/card_bashfeature.csv',index=True)\n",
    "card = pd.read_csv('input/card_bashfeature.csv') \n",
    "card=card.rename(columns={'pos' : 'price_count'}) \n",
    "\n",
    "train_test = pd.merge(train_test, card, how='left',on='id') #2512\n",
    "del card\n",
    "\n",
    "'''\n",
    "## bad feature\n",
    "## recharge feature\n",
    "recharge=card_train_test[(card_train_test.pos=='卡充值') | (card_train_test.pos=='圈存转账') ]\n",
    "\n",
    "card_recharge = pd.DataFrame(recharge.groupby(['id'])['pos'].count())\n",
    "\n",
    "card_recharge['recharge_sum']=recharge.groupby(['id'])['price'].sum()\n",
    "card_recharge['recharge_avg']=recharge.groupby(['id'])['price'].mean()\n",
    "#\n",
    "card_recharge['recharge_max']=recharge.groupby(['id'])['price'].max()\n",
    "card_recharge['recharge_min']=recharge.groupby(['id'])['price'].min()\n",
    "#\n",
    "card_recharge['recharge_median']=recharge.groupby(['id'])['price'].median() #2330\n",
    "\n",
    "\n",
    "del recharge\n",
    "card_recharge.to_csv('input/card_rechargefeature.csv',index=True)\n",
    "card_recharge = pd.read_csv('input/card_rechargefeature.csv') \n",
    "card_recharge=card_recharge.rename(columns={'pos' : 'recharge_count'}) \n",
    "train_test = pd.merge(train_test, card_recharge, how='left',on='id') \n",
    "del card_recharge\n",
    "\n",
    "## 支付领取项\n",
    "zhifu = card_train_test[card_train_test.pos=='支付领取']\n",
    "card_zhifu= pd.DataFrame(zhifu.groupby(['id'])['pos'].count())\n",
    "\n",
    "card_zhifu['zhifu_sum'] = zhifu.groupby(['id'])['price'].sum()\n",
    "card_zhifu['zhifu_avg'] = zhifu.groupby(['id'])['price'].mean()\n",
    "#\n",
    "card_zhifu['zhifu_max'] = zhifu.groupby(['id'])['price'].max()\n",
    "card_zhifu['zhifu_min'] = zhifu.groupby(['id'])['price'].min()\n",
    "#\n",
    "card_zhifu['zhifu_median'] = zhifu.groupby(['id'])['price'].median()\n",
    "\n",
    "del zhifu\n",
    "card_zhifu.to_csv('input/card_zhifufeature.csv',index=True)\n",
    "card_zhifu = pd.read_csv('input/card_zhifufeature.csv') \n",
    "card_zhifu=card_zhifu.rename(columns={'pos' : 'zhifu_count'}) \n",
    "\n",
    "\n",
    "train_test = pd.merge(train_test, card_zhifu, how='left',on='id')\n",
    "del card_zhifu\n",
    "'''\n",
    "\n",
    "## consume feature\n",
    "consume=card_train_test[card_train_test.pos == 'POS消费']\n",
    "\n",
    "card_consume = pd.DataFrame(consume.groupby(['id'])['pos'].count())\n",
    "\n",
    "card_consume['consume_sum']=consume.groupby(['id'])['price'].sum()\n",
    "card_consume['consume_avg']=consume.groupby(['id'])['price'].mean()\n",
    "card_consume['consume_max']=consume.groupby(['id'])['price'].max()\n",
    "card_consume['consume_min']=consume.groupby(['id'])['price'].min()\n",
    "card_consume['consume_median']=consume.groupby(['id'])['price'].median()\n",
    "\n",
    "del consume\n",
    "card_consume.to_csv('input/card_consumefeature.csv',index=True)\n",
    "card_consume = pd.read_csv('input/card_consumefeature.csv') \n",
    "card_consume=card_consume.rename(columns={'pos' : 'consume_count'}) \n",
    "\n",
    "\n",
    "train_test = pd.merge(train_test, card_consume, how='left',on='id') \n",
    "del card_consume\n",
    "\n",
    "kapiankaihu=card_train_test[card_train_test.pos=='卡片开户']\n",
    "card_kaihu = pd.DataFrame(kapiankaihu.groupby(['id'])['pos'].count())\n",
    "del kapiankaihu\n",
    "card_kaihu.to_csv('input/card_kaihufeature.csv',index=True)\n",
    "card_kaihu = pd.read_csv('input/card_kaihufeature.csv') \n",
    "card_kaihu=card_kaihu.rename(columns={'pos' : 'kaihu_count'}) \n",
    "\n",
    "train_test = pd.merge(train_test, card_kaihu, how='left',on='id') \n",
    "del card_kaihu\n",
    "\n",
    "kapianxiaohu=card_train_test[card_train_test.pos=='卡片销户']\n",
    "card_xiaohu = pd.DataFrame(kapianxiaohu.groupby(['id'])['pos'].count())\n",
    "del kapianxiaohu\n",
    "card_xiaohu.to_csv('input/card_xiaohufeature.csv',index=True)\n",
    "card_xiaohu = pd.read_csv('input/card_xiaohufeature.csv') \n",
    "card_xiaohu=card_xiaohu.rename(columns={'pos' : 'xiaohu_count'}) \n",
    "\n",
    "train_test = pd.merge(train_test, card_xiaohu, how='left',on='id') \n",
    "del card_xiaohu\n",
    "\n",
    "kapianbuban=card_train_test[card_train_test.pos=='卡补办']\n",
    "card_buban = pd.DataFrame(kapianbuban.groupby(['id'])['pos'].count())\n",
    "del kapianbuban\n",
    "card_buban.to_csv('input/card_bubanfeature.csv',index=True)\n",
    "card_buban = pd.read_csv('input/card_bubanfeature.csv') \n",
    "card_buban=card_buban.rename(columns={'pos' : 'buban_count'}) \n",
    "\n",
    "train_test = pd.merge(train_test, card_buban, how='left',on='id') \n",
    "del card_buban\n",
    "\n",
    "kapianjiegua=card_train_test[card_train_test.pos=='卡解挂']\n",
    "card_jiegua = pd.DataFrame(kapianjiegua.groupby(['id'])['pos'].count())\n",
    "del kapianjiegua\n",
    "card_jiegua.to_csv('input/card_jieguafeature.csv',index=True)\n",
    "card_jiegua = pd.read_csv('input/card_jieguafeature.csv') \n",
    "card_jiegua=card_jiegua.rename(columns={'pos' : 'jiegua_count'}) \n",
    "\n",
    "train_test = pd.merge(train_test, card_jiegua, how='left',on='id') \n",
    "del card_jiegua\n",
    "\n",
    "kapianchange=card_train_test[card_train_test.pos=='换卡']\n",
    "card_change = pd.DataFrame(kapianchange.groupby(['id'])['pos'].count())\n",
    "del kapianchange\n",
    "card_change.to_csv('input/card_changefeature.csv',index=True)\n",
    "card_change = pd.read_csv('input/card_changefeature.csv') \n",
    "card_change=card_change.rename(columns={'pos' : 'change_count'}) \n",
    "\n",
    "train_test = pd.merge(train_test, card_change, how='left',on='id') \n",
    "del card_change\n",
    "\n",
    "\n",
    "\n",
    "canteen=card_train_test[card_train_test.consume=='食堂']\n",
    "\n",
    "card_canteen = pd.DataFrame(canteen.groupby(['id'])['pos'].count())\n",
    "card_canteen['canteen_sum']=canteen.groupby(['id'])['price'].sum()\n",
    "card_canteen['canteen_avg']=canteen.groupby(['id'])['price'].mean()\n",
    "card_canteen['canteen_max']=canteen.groupby(['id'])['price'].max()\n",
    "card_canteen['canteen_min']=canteen.groupby(['id'])['price'].min()\n",
    "card_canteen['canteen_median']=canteen.groupby(['id'])['price'].median()\n",
    "\n",
    "del canteen\n",
    "card_canteen.to_csv('input/card_canteenfeature.csv',index=True)\n",
    "card_canteen = pd.read_csv('input/card_canteenfeature.csv') \n",
    "card_canteen=card_canteen.rename(columns={'pos' : 'canteen_count'}) \n",
    "\n",
    "train_test = pd.merge(train_test, card_canteen, how='left',on='id') \n",
    "del card_canteen\n",
    "\n",
    "boiled_water=card_train_test[card_train_test.consume=='开水']\n",
    "\n",
    "card_boiled_water = pd.DataFrame(boiled_water.groupby(['id'])['pos'].count())\n",
    "card_boiled_water['boiled_water_sum']=boiled_water.groupby(['id'])['price'].sum()\n",
    "card_boiled_water['boiled_water_avg']=boiled_water.groupby(['id'])['price'].mean()\n",
    "card_boiled_water['boiled_water_max']=boiled_water.groupby(['id'])['price'].max()\n",
    "card_boiled_water['boiled_water_min']=boiled_water.groupby(['id'])['price'].min()\n",
    "card_boiled_water['boiled_water_median']=boiled_water.groupby(['id'])['price'].median()\n",
    "\n",
    "del boiled_water\n",
    "card_boiled_water.to_csv('input/card_boiled_waterfeature.csv',index=True)\n",
    "card_boiled_water = pd.read_csv('input/card_boiled_waterfeature.csv') \n",
    "card_boiled_water=card_boiled_water.rename(columns={'pos' : 'boiled_water_count'}) \n",
    "\n",
    "train_test = pd.merge(train_test, card_boiled_water, how='left',on='id') \n",
    "del card_boiled_water\n",
    "\n",
    "bathe=card_train_test[card_train_test.consume=='淋浴']\n",
    "\n",
    "card_bathe = pd.DataFrame(bathe.groupby(['id'])['pos'].count())\n",
    "card_bathe['bathe_sum']=bathe.groupby(['id'])['price'].sum()\n",
    "card_bathe['bathe_avg']=bathe.groupby(['id'])['price'].mean()\n",
    "card_bathe['bathe_max']=bathe.groupby(['id'])['price'].max()\n",
    "card_bathe['bathe_min']=bathe.groupby(['id'])['price'].min()\n",
    "card_bathe['bathe_median']=bathe.groupby(['id'])['price'].median()\n",
    "\n",
    "del bathe\n",
    "card_bathe.to_csv('input/card_bathefeature.csv',index=True)\n",
    "card_bathe = pd.read_csv('input/card_bathefeature.csv') \n",
    "card_bathe=card_bathe.rename(columns={'pos' : 'bathe_count'}) \n",
    "\n",
    "train_test = pd.merge(train_test, card_bathe, how='left',on='id') \n",
    "del card_bathe\n",
    "\n",
    "shool_bus=card_train_test[card_train_test.consume=='校车']\n",
    "\n",
    "card_shool_bus = pd.DataFrame(shool_bus.groupby(['id'])['pos'].count())\n",
    "card_shool_bus['shool_bus_sum']=shool_bus.groupby(['id'])['price'].sum()\n",
    "card_shool_bus['shool_bus_avg']=shool_bus.groupby(['id'])['price'].mean()\n",
    "card_shool_bus['shool_bus_max']=shool_bus.groupby(['id'])['price'].max()\n",
    "card_shool_bus['shool_bus_min']=shool_bus.groupby(['id'])['price'].min()\n",
    "card_shool_bus['shool_bus_median']=shool_bus.groupby(['id'])['price'].median()\n",
    "\n",
    "del shool_bus\n",
    "card_shool_bus.to_csv('input/card_shool_busfeature.csv',index=True)\n",
    "card_shool_bus = pd.read_csv('input/card_shool_busfeature.csv') \n",
    "card_shool_bus=card_shool_bus.rename(columns={'pos' : 'shool_bus_count'}) \n",
    "\n",
    "train_test = pd.merge(train_test, card_shool_bus, how='left',on='id') \n",
    "del card_shool_bus\n",
    "\n",
    "shop=card_train_test[card_train_test.consume=='超市']\n",
    "\n",
    "card_shop = pd.DataFrame(shop.groupby(['id'])['pos'].count())\n",
    "card_shop['shop_sum']=shop.groupby(['id'])['price'].sum()\n",
    "card_shop['shop_avg']=shop.groupby(['id'])['price'].mean()\n",
    "card_shop['shop_max']=shop.groupby(['id'])['price'].max()\n",
    "card_shop['shop_min']=shop.groupby(['id'])['price'].min()\n",
    "card_shop['shop_median']=shop.groupby(['id'])['price'].median()\n",
    "\n",
    "del shop\n",
    "card_shop.to_csv('input/card_shopfeature.csv',index=True)\n",
    "card_shop = pd.read_csv('input/card_shopfeature.csv') \n",
    "card_shop=card_shop.rename(columns={'pos' : 'shop_count'}) \n",
    "\n",
    "train_test = pd.merge(train_test, card_shop, how='left',on='id') \n",
    "del card_shop\n",
    "\n",
    "wash_house=card_train_test[card_train_test.consume=='洗衣房']\n",
    "\n",
    "card_wash_house = pd.DataFrame(wash_house.groupby(['id'])['pos'].count())\n",
    "card_wash_house['wash_sum']=wash_house.groupby(['id'])['price'].sum()\n",
    "card_wash_house['wash_avg']=wash_house.groupby(['id'])['price'].mean()\n",
    "card_wash_house['wash_max']=wash_house.groupby(['id'])['price'].max()\n",
    "card_wash_house['wash_min']=wash_house.groupby(['id'])['price'].min()\n",
    "card_wash_house['wash_median']=wash_house.groupby(['id'])['price'].median()\n",
    "\n",
    "del wash_house\n",
    "card_wash_house.to_csv('input/card_wash_housefeature.csv',index=True)\n",
    "card_wash_house = pd.read_csv('input/card_wash_housefeature.csv') \n",
    "card_wash_house=card_wash_house.rename(columns={'pos' : 'wash_house_count'}) \n",
    "\n",
    "train_test = pd.merge(train_test, card_wash_house, how='left',on='id') \n",
    "del card_wash_house\n",
    "\n",
    "library=card_train_test[card_train_test.consume=='图书馆']\n",
    "\n",
    "card_library= pd.DataFrame(library.groupby(['id'])['pos'].count())\n",
    "card_library['library_sum']=library.groupby(['id'])['price'].sum()\n",
    "card_library['library_avg']=library.groupby(['id'])['price'].mean()\n",
    "card_library['library_max']=library.groupby(['id'])['price'].max()\n",
    "card_library['library_min']=library.groupby(['id'])['price'].min()\n",
    "card_library['library_median']=library.groupby(['id'])['price'].median()\n",
    "\n",
    "del library\n",
    "card_library.to_csv('input/card_libraryfeature.csv',index=True)\n",
    "card_library = pd.read_csv('input/card_libraryfeature.csv') \n",
    "card_library=card_library.rename(columns={'pos' : 'library_count'}) \n",
    "\n",
    "train_test = pd.merge(train_test, card_library, how='left',on='id') \n",
    "del card_library\n",
    "\n",
    "printhouse=card_train_test[card_train_test.consume=='文印中心']\n",
    "\n",
    "card_printhouse= pd.DataFrame(printhouse.groupby(['id'])['pos'].count())\n",
    "card_printhouse['print_sum']=printhouse.groupby(['id'])['price'].sum()\n",
    "card_printhouse['print_avg']=printhouse.groupby(['id'])['price'].mean()\n",
    "card_printhouse['print_max']=printhouse.groupby(['id'])['price'].max()\n",
    "card_printhouse['print_min']=printhouse.groupby(['id'])['price'].min()\n",
    "card_printhouse['print_median']=printhouse.groupby(['id'])['price'].median()\n",
    "\n",
    "del printhouse\n",
    "card_printhouse.to_csv('input/card_printhousefeature.csv',index=True)\n",
    "card_printhouse = pd.read_csv('input/card_printhousefeature.csv') \n",
    "card_printhouse=card_printhouse.rename(columns={'pos' : 'printhouse_count'}) \n",
    "\n",
    "train_test = pd.merge(train_test, card_printhouse, how='left',on='id') \n",
    "del card_printhouse\n",
    "\n",
    "dean=card_train_test[card_train_test.consume=='教务处']\n",
    "\n",
    "card_dean= pd.DataFrame(dean.groupby(['id'])['pos'].count())\n",
    "card_dean['dean_sum']=dean.groupby(['id'])['price'].sum()\n",
    "card_dean['dean_avg']=dean.groupby(['id'])['price'].mean()\n",
    "card_dean['dean_max']=dean.groupby(['id'])['price'].max()\n",
    "card_dean['dean_min']=dean.groupby(['id'])['price'].min()\n",
    "card_dean['dean_median']=dean.groupby(['id'])['price'].median()\n",
    "\n",
    "del dean\n",
    "card_dean.to_csv('input/card_deanfeature.csv',index=True)\n",
    "card_dean = pd.read_csv('input/card_deanfeature.csv') \n",
    "card_dean=card_dean.rename(columns={'pos' : 'dean_count'}) \n",
    "\n",
    "train_test = pd.merge(train_test, card_dean, how='left',on='id') \n",
    "del card_dean\n",
    "\n",
    "other=card_train_test[card_train_test.consume=='其他']\n",
    "\n",
    "card_other= pd.DataFrame(other.groupby(['id'])['pos'].count())\n",
    "card_other['other_sum']=other.groupby(['id'])['price'].sum()\n",
    "#\n",
    "card_other['other_avg']=other.groupby(['id'])['price'].mean()\n",
    "#\n",
    "card_other['other_max']=other.groupby(['id'])['price'].max()\n",
    "#\n",
    "card_other['other_min']=other.groupby(['id'])['price'].min()\n",
    "#\n",
    "card_other['other_median']=other.groupby(['id'])['price'].median()\n",
    "\n",
    "del other\n",
    "card_other.to_csv('input/card_otherfeature.csv',index=True)\n",
    "card_other = pd.read_csv('input/card_otherfeature.csv') \n",
    "card_other=card_other.rename(columns={'pos' : 'other_count'}) \n",
    "\n",
    "train_test = pd.merge(train_test, card_other, how='left',on='id') \n",
    "del card_other\n",
    "\n",
    "\n",
    "hospital=card_train_test[card_train_test.consume=='校医院']\n",
    "\n",
    "card_hospital= pd.DataFrame(hospital.groupby(['id'])['pos'].count())\n",
    "card_hospital['hospital_sum']=hospital.groupby(['id'])['price'].sum()\n",
    "card_hospital['hospital_avg']=hospital.groupby(['id'])['price'].mean()\n",
    "card_hospital['hospital_max']=hospital.groupby(['id'])['price'].max()\n",
    "card_hospital['hospital_min']=hospital.groupby(['id'])['price'].min()\n",
    "card_hospital['hospital_median']=hospital.groupby(['id'])['price'].median()\n",
    "\n",
    "del hospital\n",
    "card_hospital.to_csv('input/card_hospitalfeature.csv',index=True)\n",
    "card_hospital = pd.read_csv('input/card_hospitalfeature.csv') \n",
    "card_hospital=card_hospital.rename(columns={'pos' : 'hospital_count'}) \n",
    "\n",
    "train_test = pd.merge(train_test, card_hospital, how='left',on='id') \n",
    "del card_hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test=pd.read_csv('input/train_time.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#del card_train_test\n",
    "train_test=train_test.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>college</th>\n",
       "      <th>rank</th>\n",
       "      <th>total_people</th>\n",
       "      <th>rank_percent</th>\n",
       "      <th>countM1</th>\n",
       "      <th>price_sumM1</th>\n",
       "      <th>price_avgM1</th>\n",
       "      <th>price_maxM1</th>\n",
       "      <th>...</th>\n",
       "      <th>地点263_avg</th>\n",
       "      <th>地点263_max</th>\n",
       "      <th>地点263_min</th>\n",
       "      <th>地点263_median</th>\n",
       "      <th>地点840_count</th>\n",
       "      <th>地点840_sum</th>\n",
       "      <th>地点840_avg</th>\n",
       "      <th>地点840_max</th>\n",
       "      <th>地点840_min</th>\n",
       "      <th>地点840_median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2933.0</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>49.0</td>\n",
       "      <td>201.31</td>\n",
       "      <td>4.108367</td>\n",
       "      <td>36.4</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2933.0</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>6.157895</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1565.0</td>\n",
       "      <td>1570.0</td>\n",
       "      <td>0.996815</td>\n",
       "      <td>97.0</td>\n",
       "      <td>347.74</td>\n",
       "      <td>3.584948</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1570.0</td>\n",
       "      <td>1570.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>98.0</td>\n",
       "      <td>491.01</td>\n",
       "      <td>5.010306</td>\n",
       "      <td>17.5</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2304.0</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>27.0</td>\n",
       "      <td>82.96</td>\n",
       "      <td>3.072593</td>\n",
       "      <td>22.3</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 432 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label  college    rank  total_people  rank_percent  countM1  \\\n",
       "0   0    0.0      9.0     1.0        2933.0      0.000341     49.0   \n",
       "1   1    0.0      9.0     2.0        2933.0      0.000682     -1.0   \n",
       "2   8    0.0      6.0  1565.0        1570.0      0.996815     97.0   \n",
       "3   9    0.0      6.0  1570.0        1570.0      1.000000     98.0   \n",
       "4  10    0.0      3.0     1.0        2304.0      0.000434     27.0   \n",
       "\n",
       "   price_sumM1  price_avgM1  price_maxM1      ...       地点263_avg  地点263_max  \\\n",
       "0       201.31     4.108367         36.4      ...            -1.0       -1.0   \n",
       "1        -1.00    -1.000000         -1.0      ...            -1.0       -1.0   \n",
       "2       347.74     3.584948         10.0      ...            -1.0       -1.0   \n",
       "3       491.01     5.010306         17.5      ...            -1.0       -1.0   \n",
       "4        82.96     3.072593         22.3      ...            -1.0       -1.0   \n",
       "\n",
       "   地点263_min  地点263_median  地点840_count  地点840_sum  地点840_avg  地点840_max  \\\n",
       "0       -1.0          -1.0         -1.0       -1.0  -1.000000       -1.0   \n",
       "1       -1.0          -1.0         19.0      117.0   6.157895        7.0   \n",
       "2       -1.0          -1.0         -1.0       -1.0  -1.000000       -1.0   \n",
       "3       -1.0          -1.0         -1.0       -1.0  -1.000000       -1.0   \n",
       "4       -1.0          -1.0          2.0        7.0   3.500000        4.0   \n",
       "\n",
       "   地点840_min  地点840_median  \n",
       "0       -1.0          -1.0  \n",
       "1        4.0           6.0  \n",
       "2       -1.0          -1.0  \n",
       "3       -1.0          -1.0  \n",
       "4        3.0           3.5  \n",
       "\n",
       "[5 rows x 432 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Evaluation \n",
    "def f1_macro(label_truth, predictions):\n",
    "    df=pd.DataFrame(columns=[\"subsidy_x\",\"subsidy_y\"])\n",
    "    df.subsidy_y=predictions\n",
    "    df.subsidy_x=np.array(label_truth)\n",
    "    df.subsidy_y = df.subsidy_y.apply(lambda x:int(x))\n",
    "\n",
    "    \n",
    "    correct = df[df['subsidy_x'] == df['subsidy_y']]\n",
    "    s = 0\n",
    "    for i in [1000, 1500, 2000]:\n",
    "        print '\\n%d'%i\n",
    "        if sum(df['subsidy_x'] == i)!=0:\n",
    "            r = float(sum(correct['subsidy_y'] == i))/sum(df['subsidy_x'] == i)\n",
    "            print 'Recall---%s'%r\n",
    "        else: \n",
    "            r=0\n",
    "        if sum(df['subsidy_y'] == i)!=0:\n",
    "            p = float(sum(correct['subsidy_y'] == i))/sum(df['subsidy_y'] == i)        \n",
    "            print 'Precision---%s'%p\n",
    "        else:\n",
    "            p=0\n",
    "        if (r+p)!=0:\n",
    "            f = r*p*2/(r+p)\n",
    "            print 'F1---%s'%f\n",
    "        if not np.isnan(f):\n",
    "            s += (float(sum(df['subsidy_x'] == i))/df.shape[0])*f\n",
    "    \n",
    "    print '\\nF1-macro---%s'%s\n",
    "    return s\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nice_feature=pd.read_csv('input/nice_feature.csv',header=None,index_col=0)\n",
    "\n",
    "target = 'label'\n",
    "IDcol = 'id'\n",
    "ids = test['id'].values\n",
    "\n",
    "all_feature = [x for x in train.columns if x not in [target]]\n",
    "\n",
    "#predictors = [ x for x in all_feature if x in all_feature]\n",
    "#predictors = [ x for x in all_feature if x in nice_feature.index]\n",
    "\n",
    "feature_imp_place20=pd.read_csv('input/feature_imp_place20.csv')\n",
    "\n",
    "predictors = [ x for x in all_feature if (x in nice_feature.index)|(x in feature_imp_place20.feature.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = train_test.ix[:, train_test.columns != 'label']\n",
    "y = train_test.ix[:, train_test.columns == 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuhung/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of normal transactions: ', 1089)\n",
      "('Number of fraud transactions: ', 1089)\n",
      "('Total number of transactions in resampled data: ', 2178)\n"
     ]
    }
   ],
   "source": [
    "train=X_train\n",
    "train['label']=y_train\n",
    "\n",
    "test=X_test\n",
    "\n",
    "y_train['label'].value_counts()\n",
    "\n",
    "\n",
    "# tips：\n",
    "# 尝试欠采样和smoth采样\n",
    "\n",
    "# undersample\n",
    "train.to_csv('input/train_7r07part.csv',index=False)\n",
    "train=pd.read_csv('input/train_7r07part.csv')\n",
    "\n",
    "number_records_fraud = len(train[train.label != 0])\n",
    "fraud_indices = np.array(train[train.label != 0].index)\n",
    "\n",
    "# Picking the indices of the normal classes\n",
    "normal_indices = train[train.label == 0].index\n",
    "\n",
    "# Out of the indices we picked, randomly select \"x\" number (number_records_fraud)\n",
    "random_normal_indices = np.random.choice(normal_indices, number_records_fraud, replace = False)\n",
    "random_normal_indices = np.array(random_normal_indices)\n",
    "\n",
    "# Appending the 2 indices\n",
    "under_sample_indices = np.concatenate([fraud_indices,random_normal_indices])\n",
    "\n",
    "# Under sample dataset\n",
    "under_sample_data = train.iloc[under_sample_indices,:]\n",
    "\n",
    "X_undersample = under_sample_data.ix[:, under_sample_data.columns != 'label']\n",
    "y_undersample = under_sample_data.ix[:, under_sample_data.columns == 'label']\n",
    "\n",
    "# Showing ratio\n",
    "print(\"Number of normal transactions: \", len(under_sample_data[under_sample_data.label == 0]))\n",
    "print(\"Number of fraud transactions: \", len(under_sample_data[under_sample_data.label != 0]))\n",
    "print(\"Total number of transactions in resampled data: \", len(under_sample_data))\n",
    "\n",
    "\n",
    "# Oversample\n",
    "Oversampling1000 = train.loc[train.label == 1000]\n",
    "Oversampling1500 = train.loc[train.label == 1500]\n",
    "Oversampling2000 = train.loc[train.label == 2000]\n",
    "\n",
    "for i in range(5):\n",
    "    train = train.append(Oversampling1000)\n",
    "for j in range(7):\n",
    "    train = train.append(Oversampling1500)\n",
    "for k in range(9):\n",
    "    train = train.append(Oversampling2000)\n",
    "'''\n",
    "for i in range(5):\n",
    "    train = train.append(Oversampling1000)\n",
    "for j in range(8):\n",
    "    train = train.append(Oversampling1500)\n",
    "for k in range(10):\n",
    "    train = train.append(Oversampling2000)\n",
    "\n",
    "'''   \n",
    "train = train[train['label'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14690, 432)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1000\n",
      "Recall---0.380530973451\n",
      "Precision---0.19153674833\n",
      "F1---0.254814814815\n",
      "\n",
      "1500\n",
      "Recall---0.215384615385\n",
      "Precision---0.141414141414\n",
      "F1---0.170731707317\n",
      "\n",
      "2000\n",
      "Recall---0.208695652174\n",
      "Precision---0.140350877193\n",
      "F1---0.167832167832\n",
      "\n",
      "F1-macro---0.0303380187998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.030338018799775556"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf=XGBClassifier(max_depth=4,objective='multi:softmax',n_estimators=100,seed=42)\n",
    "clf = clf.fit(train[predictors],train[target])\n",
    "result = clf.predict(test[predictors])\n",
    "\n",
    "f1_macro(y_test,result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "1000\n",
    "Recall---0.544247787611\n",
    "Precision---0.188940092166\n",
    "F1---0.280501710376\n",
    "\n",
    "1500\n",
    "Recall---0.207692307692\n",
    "Precision---0.151685393258\n",
    "F1---0.175324675325\n",
    "\n",
    "2000\n",
    "Recall---0.191304347826\n",
    "Precision---0.164179104478\n",
    "F1---0.176706827309\n",
    "\n",
    "F1-macro---0.0326108020446\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "r_scale=preprocessing.RobustScaler()\n",
    "\n",
    "train_scaled=r_scale.fit_transform(train[predictors])\n",
    "test_scaled=r_scale.transform(test[predictors])\n",
    "\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "# model\n",
    "# 对照组\n",
    "clf=XGBClassifier(max_depth=4,objective='multi:softmax',n_estimators=100,seed=42)\n",
    "clf = clf.fit(train[predictors],train[target])\n",
    "result = clf.predict(test[predictors])\n",
    "print '\\n--------XGBClassifier--------'\n",
    "f1_macro(y_test,result)\n",
    "\n",
    "clf=XGBClassifier(max_depth=4,objective='multi:softmax',n_estimators=180,seed=42,learning_rate=0.05)\n",
    "clf = clf.fit(train[predictors],train[target])\n",
    "result = clf.predict(test[predictors])\n",
    "print '\\n--------XGBClassifier(learning_rate=0.05)--------'\n",
    "f1_macro(y_test,result)\n",
    "\n",
    "clf=XGBClassifier(max_depth=4,objective='multi:softmax',n_estimators=100,seed=42)\n",
    "clf = clf.fit(X_undersample[predictors],y_undersample[target])\n",
    "result = clf.predict(test[predictors])\n",
    "print '\\n--------XGBClassifier(undersample)--------'\n",
    "f1_macro(y_test,result)\n",
    "\n",
    "\n",
    "clf=XGBClassifier(max_depth=4,objective='multi:softmax',n_estimators=100,seed=42)\n",
    "clf = clf.fit(np.log(train[predictors]),train[target])\n",
    "result = clf.predict(np.log(test[predictors]))\n",
    "print '\\n--------XGBClassifier(log)--------'\n",
    "f1_macro(y_test,result)\n",
    "\n",
    "\n",
    "clf=XGBClassifier(max_depth=4,objective='multi:softmax',n_estimators=100,seed=42)\n",
    "clf = clf.fit(train_scaled,train[target])\n",
    "result = clf.predict(test_scaled)\n",
    "print '\\n--------XGBClassifier(scaled)--------'\n",
    "f1_macro(y_test,result)\n",
    "\n",
    "# model\n",
    "#\n",
    "\n",
    "clf = GradientBoostingClassifier(random_state=42,n_estimators=200)\n",
    "clf = clf.fit(train[predictors],train[target])\n",
    "result = clf.predict(test[predictors])\n",
    "print '\\n--------GradientBoosting--------'\n",
    "f1_macro(y_test,result)\n",
    "\n",
    "clf = GradientBoostingClassifier(random_state=42,n_estimators=200)\n",
    "clf = clf.fit(X_undersample[predictors],y_undersample[target])\n",
    "result = clf.predict(test[predictors])\n",
    "print '\\n--------GradientBoosting(undersample)--------'\n",
    "f1_macro(y_test,result)\n",
    "\n",
    "# GBC log会报错\n",
    "#clf = GradientBoostingClassifier(random_state=42,n_estimators=200)\n",
    "#clf = clf.fit(np.log(train[predictors]),train[target])\n",
    "#result = clf.predict(np.log(test[predictors]))\n",
    "#print '\\n--------GradientBoosting(log)--------'\n",
    "#f1_macro(y_test,result)\n",
    "\n",
    "clf = GradientBoostingClassifier(random_state=42,n_estimators=200)\n",
    "clf = clf.fit(train_scaled,train[target])\n",
    "result = clf.predict(test_scaled)\n",
    "print '\\n--------GradientBoosting(scaled)--------'\n",
    "f1_macro(y_test,result)\n",
    "\n",
    "\n",
    "\n",
    "# \n",
    "clf = RandomForestClassifier()\n",
    "clf = clf.fit(train[predictors],train[target])\n",
    "result = clf.predict(test[predictors])\n",
    "print '\\n--------RFClassifier--------'\n",
    "f1_macro(y_test,result)\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf = clf.fit(X_undersample[predictors],y_undersample[target])\n",
    "result = clf.predict(test[predictors])\n",
    "print '\\n--------RFClassifier(undersample)--------'\n",
    "f1_macro(y_test,result)\n",
    "\n",
    "# log 报错\n",
    "#clf = RandomForestClassifier()\n",
    "#clf = clf.fit(np.log(train[predictors]),train[target])\n",
    "#result = clf.predict(np.log(test[predictors]))\n",
    "#print '\\n--------RFClassifier(log)--------'\n",
    "#f1_macro(y_test,result)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf = clf.fit(train_scaled,train[target])\n",
    "result = clf.predict(test_scaled)\n",
    "print '\\n--------RFClassifier(scaled)--------'\n",
    "f1_macro(y_test,result)\n",
    "\n",
    "\n",
    "#f1_macro(y_test,result)\n",
    "#plot_confusion_matrix(confusion_matrix(y_test,result),classes=[0,1000,1500,2000])\n",
    "\n",
    "# tips：\n",
    "# 尝试用二分类（得与不得，得多少的分类）\n",
    "# 尝试神经网络\n",
    "# 模型调参(常规参数与阈值)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 由于聚类的精度不如逻辑回归二分类，所以todo is\n",
    "# 将结果进行二分类后，再对分类结果为正的进行训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# else trick\\n\\ndef model_f1_macro(name,clf):\\n    clf=clf\\n    clf = clf.fit(train[predictors],train[target])\\n    result = clf.predict(test[predictors])\\n    print '\\n--------%s--------'%name\\n    f1_macro(y_test,result)\\n    \\nfrom sklearn.cross_validation import cross_val_score\\n\\n# meta-estimator\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.ensemble import ExtraTreesClassifier\\nfrom sklearn.ensemble import AdaBoostClassifier\\nfrom sklearn.ensemble import GradientBoostingClassifier \\n\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\\n\\n\\nclassifiers = {\\n    'KN': KNeighborsClassifier(3),\\n    'DT': DecisionTreeClassifier(max_depth=5),\\n    'RF': RandomForestClassifier(n_estimators=10, max_depth=5, max_features=1),  # clf.feature_importances_\\n    'ET': ExtraTreesClassifier(n_estimators=10, max_depth=None),  # clf.feature_importances_\\n    'AB': AdaBoostClassifier(n_estimators=100),\\n    'GB': GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0), # clf.feature_importances_\\n    'GNB': GaussianNB(),\\n    'LD': LinearDiscriminantAnalysis(),\\n    'QD': QuadraticDiscriminantAnalysis()}\\n\\n    \\n    \\nX, y = train[predictors],train[target]\\n\\nfor name, clf in classifiers.items():\\n    scores = cross_val_score(clf, X, y)\\n    print '\\n',name,'\\t--> ',scores.mean()\\n    model_f1_macro(name,clf)\\n    \\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# else trick\n",
    "\n",
    "def model_f1_macro(name,clf):\n",
    "    clf=clf\n",
    "    clf = clf.fit(train[predictors],train[target])\n",
    "    result = clf.predict(test[predictors])\n",
    "    print '\\n--------%s--------'%name\n",
    "    f1_macro(y_test,result)\n",
    "    \n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "# meta-estimator\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "\n",
    "classifiers = {\n",
    "    'KN': KNeighborsClassifier(3),\n",
    "    'DT': DecisionTreeClassifier(max_depth=5),\n",
    "    'RF': RandomForestClassifier(n_estimators=10, max_depth=5, max_features=1),  # clf.feature_importances_\n",
    "    'ET': ExtraTreesClassifier(n_estimators=10, max_depth=None),  # clf.feature_importances_\n",
    "    'AB': AdaBoostClassifier(n_estimators=100),\n",
    "    'GB': GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0), # clf.feature_importances_\n",
    "    'GNB': GaussianNB(),\n",
    "    'LD': LinearDiscriminantAnalysis(),\n",
    "    'QD': QuadraticDiscriminantAnalysis()}\n",
    "\n",
    "    \n",
    "    \n",
    "X, y = train[predictors],train[target]\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    scores = cross_val_score(clf, X, y)\n",
    "    print '\\n',name,'\\t--> ',scores.mean()\n",
    "    model_f1_macro(name,clf)\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n## vote\\nfrom sklearn import ensemble\\n\\nclf1 = XGBClassifier(max_depth=4,objective='multi:softmax',n_estimators=100,seed=42)\\nclf2 = XGBClassifier(max_depth=3,objective='multi:softmax',n_estimators=100,seed=42)\\n#lf3 = XGBClassifier(max_depth=4,objective='multi:softmax',n_estimators=100,seed=0)\\n#clf4 = XGBClassifier(max_depth=3,objective='multi:softmax',n_estimators=100,seed=0)\\n\\nclf3 = GradientBoostingClassifier(random_state=42)\\nclf4 = GradientBoostingClassifier(n_estimators=200,random_state=42)\\n\\nclfs=ensemble.VotingClassifier(estimators=[('xgb1',clf1),('xgb2',clf2),('GBM',clf3),('RF',clf4)],voting='hard')\\nclfs = clfs.fit(train[predictors],train[target])\\n#clfs = clfs.fit(X_undersample[predictors],y_undersample[target])\\n\\nresult = clfs.predict(test[predictors])\\nprint '\\n--------Vote--------'\\nf1_macro(y_test,result)\\n\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "## vote\n",
    "from sklearn import ensemble\n",
    "\n",
    "clf1 = XGBClassifier(max_depth=4,objective='multi:softmax',n_estimators=100,seed=42)\n",
    "clf2 = XGBClassifier(max_depth=3,objective='multi:softmax',n_estimators=100,seed=42)\n",
    "#lf3 = XGBClassifier(max_depth=4,objective='multi:softmax',n_estimators=100,seed=0)\n",
    "#clf4 = XGBClassifier(max_depth=3,objective='multi:softmax',n_estimators=100,seed=0)\n",
    "\n",
    "clf3 = GradientBoostingClassifier(random_state=42)\n",
    "clf4 = GradientBoostingClassifier(n_estimators=200,random_state=42)\n",
    "\n",
    "clfs=ensemble.VotingClassifier(estimators=[('xgb1',clf1),('xgb2',clf2),('GBM',clf3),('RF',clf4)],voting='hard')\n",
    "clfs = clfs.fit(train[predictors],train[target])\n",
    "#clfs = clfs.fit(X_undersample[predictors],y_undersample[target])\n",
    "\n",
    "result = clfs.predict(test[predictors])\n",
    "print '\\n--------Vote--------'\n",
    "f1_macro(y_test,result)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.decomposition import PCA\\n\\npca=PCA(n_components=100)\\n\\nclf=make_pipeline(pca,GradientBoostingClassifier())\\n\\nclf = clf.fit(train[predictors],train[target])\\nresult = clf.predict(test[predictors])\\nf1_macro(y_test,result)\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca=PCA(n_components=100)\n",
    "\n",
    "clf=make_pipeline(pca,GradientBoostingClassifier())\n",
    "\n",
    "clf = clf.fit(train[predictors],train[target])\n",
    "result = clf.predict(test[predictors])\n",
    "f1_macro(y_test,result)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n## out\\xe7\\x82\\xb9\\xe5\\xa4\\x84\\xe7\\x90\\x86\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.preprocessing import StandardScaler\\n\\nclf=make_pipeline(StandardScaler(),GradientBoostingClassifier())\\n\\nclf = clf.fit(train[predictors],train[target])\\nresult = clf.predict(test[predictors])\\nf1_macro(y_test,result)\\n\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "## out点处理\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "clf=make_pipeline(StandardScaler(),GradientBoostingClassifier())\n",
    "\n",
    "clf = clf.fit(train[predictors],train[target])\n",
    "result = clf.predict(test[predictors])\n",
    "f1_macro(y_test,result)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef score(y_true,y_pred):\\n    y_true=pd.Series(y_true)\\n    y_pred=pd.Series(y_pred)\\n    count=y_true.value_counts()\\n    \\n    #y_pred=pd.Series(y_pred)\\n    #count=y_true[target].value_counts()\\n    \\n    score1000=f1_score(y_true==1000,y_pred==1000)*count[1000]/y_true.size\\n    score1500=f1_score(y_true==1500,y_pred==1500)*count[1500]/y_true.size\\n    score2000=f1_score(y_true==2000,y_pred==2000)*count[2000]/y_true.size\\n    \\n    return score1000+score1500+score2000\\n    \\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def score(y_true,y_pred):\n",
    "    y_true=pd.Series(y_true)\n",
    "    y_pred=pd.Series(y_pred)\n",
    "    count=y_true.value_counts()\n",
    "    \n",
    "    #y_pred=pd.Series(y_pred)\n",
    "    #count=y_true[target].value_counts()\n",
    "    \n",
    "    score1000=f1_score(y_true==1000,y_pred==1000)*count[1000]/y_true.size\n",
    "    score1500=f1_score(y_true==1500,y_pred==1500)*count[1500]/y_true.size\n",
    "    score2000=f1_score(y_true==2000,y_pred==2000)*count[2000]/y_true.size\n",
    "    \n",
    "    return score1000+score1500+score2000\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f1_macro0_drop(label_truth, predictions):\n",
    "    df=pd.DataFrame(columns=[\"subsidy_x\",\"subsidy_y\"])\n",
    "    df.subsidy_y=predictions\n",
    "    df.subsidy_x=np.array(label_truth)\n",
    "    df.subsidy_y = df.subsidy_y.apply(lambda x:int(x))\n",
    "\n",
    "    \n",
    "    correct = df[df['subsidy_x'] == df['subsidy_y']]\n",
    "    s = 0\n",
    "    for i in [1000, 1500, 2000]:\n",
    "        if sum(df['subsidy_x'] == i)!=0:\n",
    "            r = float(sum(correct['subsidy_y'] == i))/sum(df['subsidy_x'] == i)\n",
    "        else: \n",
    "            r=0\n",
    "        if sum(df['subsidy_y'] == i)!=0:\n",
    "            p = float(sum(correct['subsidy_y'] == i))/sum(df['subsidy_y'] == i)        \n",
    "        else:\n",
    "            p=0\n",
    "        if (r+p)!=0:\n",
    "            f = r*p*2/(r+p)\n",
    "        if not np.isnan(f):\n",
    "            s += (float(sum(df['subsidy_x'] == i))/df.shape[0])*f\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "my_score=make_scorer(score_func=f1_macro0_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuhung/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# 参数调节\n",
    "\n",
    "from sklearn import cross_validation, metrics  \n",
    "from sklearn.model_selection import GridSearchCV \n",
    "\n",
    "def modelfit(alg, dtrain, predictors, target,performCV=True, printFeatureImportance=False, cv_folds=5):\n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain[target])\n",
    "\n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "\n",
    "    #Perform cross-validation:\n",
    "    if performCV:\n",
    "        cv_score = cross_validation.cross_val_score(alg, dtrain[predictors], dtrain[target], cv=cv_folds, scoring=my_score)\n",
    "\n",
    "    #Print model report:\n",
    "    print \"\\nModel Report\"\n",
    "    print \"Accuracy : %.4g\" % metrics.accuracy_score(dtrain[target].values, dtrain_predictions)\n",
    "    \n",
    "    if performCV:\n",
    "        print \"CV Score : Mean - %.7g | Std - %.7g | Min - %.7g | Max - %.7g\" % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score))\n",
    "\n",
    "    #Print Feature Importance:\n",
    "    if printFeatureImportance:\n",
    "        feat_imp = pd.Series(alg.feature_importances_, predictors).sort_values(ascending=False)\n",
    "        feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "        plt.ylabel('Feature Importance Score')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.8567\n",
      "CV Score : Mean - 0.4730227 | Std - 0.005252156 | Min - 0.4655394 | Max - 0.4794882\n"
     ]
    }
   ],
   "source": [
    "gbm0 = GradientBoostingClassifier(random_state=10)\n",
    "modelfit(gbm0, train, predictors,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.8737\n",
      "CV Score : Mean - 0.1572691 | Std - 0.01077795 | Min - 0.1430099 | Max - 0.1729337\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-2f5be78a4b60>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m param_grid = param_test1, scoring=my_score,n_jobs=4,iid=False, cv=5)\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#gsearch1.fit(train[predictors],train[target])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mgsearch1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_undersample\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_undersample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mgsearch1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid_scores_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgsearch1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgsearch1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kuhung/anaconda2/lib/python2.7/site-packages/sklearn/model_selection/_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    938\u001b[0m             \u001b[0mtrain\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtest\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m         \"\"\"\n\u001b[1;32m--> 940\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    941\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kuhung/anaconda2/lib/python2.7/site-packages/sklearn/model_selection/_search.pyc\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, groups, parameter_iterable)\u001b[0m\n\u001b[0;32m    560\u001b[0m                                   \u001b[0mreturn_times\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m--> 562\u001b[1;33m           \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    563\u001b[0m           for train, test in cv.split(X, y, groups))\n\u001b[0;32m    564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kuhung/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 758\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kuhung/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 603\u001b[1;33m             \u001b[0mtasks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchedCalls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    604\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m                 \u001b[1;31m# No more tasks available in the iterator: tell caller to stop.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kuhung/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, iterator_slice)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator_slice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator_slice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kuhung/anaconda2/lib/python2.7/site-packages/sklearn/model_selection/_search.pyc\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m((parameters,))\u001b[0m\n\u001b[0;32m    561\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m    562\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 563\u001b[1;33m           for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[0;32m    564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m         \u001b[1;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kuhung/anaconda2/lib/python2.7/site-packages/sklearn/model_selection/_split.pyc\u001b[0m in \u001b[0;36msplit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    319\u001b[0m                                                              n_samples))\n\u001b[0;32m    320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_BaseKFold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kuhung/anaconda2/lib/python2.7/site-packages/sklearn/model_selection/_split.pyc\u001b[0m in \u001b[0;36msplit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iter_test_masks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m             \u001b[0mtrain_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogical_not\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[0mtest_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kuhung/anaconda2/lib/python2.7/site-packages/sklearn/model_selection/_split.pyc\u001b[0m in \u001b[0;36m_iter_test_masks\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_iter_test_masks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 608\u001b[1;33m         \u001b[0mtest_folds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_test_folds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    609\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mtest_folds\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kuhung/anaconda2/lib/python2.7/site-packages/sklearn/model_selection/_split.pyc\u001b[0m in \u001b[0;36m_make_test_folds\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    593\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtest_fold_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mper_cls_splits\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mper_cls_cvs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_split\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munique_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mper_cls_splits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m                 \u001b[0mcls_test_folds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_folds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    596\u001b[0m                 \u001b[1;31m# the test split can be too big because we used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m                 \u001b[1;31m# KFold(...).split(X[:max(c, n_splits)]) when data is not 100%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "## why the score of it is higher?\n",
    "## Because of resample.After resample,the dis of dataset had changed.\n",
    "# 0.02779836483035414\n",
    "\n",
    "xgb0 = XGBClassifier(max_depth=4,objective='multi:softmax',n_estimators=100,seed=42)\n",
    "modelfit(xgb0, under_sample_data, predictors,target)\n",
    "\n",
    "param_test1 = {'n_estimators':range(20,81,10)}\n",
    "gsearch1 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, min_samples_split=80,min_samples_leaf=8,max_depth=3,max_features='sqrt',subsample=0.8,random_state=10), \n",
    "param_grid = param_test1, scoring=my_score,n_jobs=4,iid=False, cv=5)\n",
    "#gsearch1.fit(train[predictors],train[target])\n",
    "gsearch1.fit(X_undersample,y_undersample)\n",
    "\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Feature importance\n",
    "'''\n",
    "clf=XGBClassifier(max_depth=4,objective='multi:softmax',n_estimators=100,seed=42)\n",
    "clf = clf.fit(train[predictors],train[target])\n",
    "result = clf.predict_proba(test[predictors])\n",
    "f1_macro(y_test,result)\n",
    "\n",
    "feat_imp=pd.Series(clf.booster().get_fscore()).sort_values(ascending=False)\n",
    "feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "feat_imp.tail(50)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "## Evaluation\n",
    "def score(df):\n",
    "    # df有三列，ID:学生ID,subsidy_x:实际奖学金金额,subsidy_y:预测奖学金金额\n",
    "    correct = test_result[test_result['subsidy_x'] == test_result['subsidy_y']]\n",
    "    s = 0\n",
    "    for i in [1000, 1500, 2000]:\n",
    "        r = float(sum(correct['subsidy_y'] == i))/sum(test_result['subsidy_x'] == i)\n",
    "        p = float(sum(correct['subsidy_y'] == i))/sum(test_result['subsidy_y'] == i)\n",
    "        f = r*p*2/(r+p)\n",
    "        if not np.isnan(f):\n",
    "            s += (float(sum(test_result['subsidy_x'] == i))/test_result.shape[0])*f\n",
    "    print(s)\n",
    "\n",
    "test_result = pd.DataFrame(columns=[\"studentid\",\"subsidy_x\",\"subsidy_y\"])\n",
    "test_result.studentid = ids\n",
    "\n",
    "test_result.subsidy_x =np.array(y_test)\n",
    "test_result.subsidy_y = result\n",
    "test_result.subsidy_y = test_result.subsidy_y.apply(lambda x:int(x))\n",
    "\n",
    "score(test_result)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
